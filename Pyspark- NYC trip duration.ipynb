{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "New York Yellow Taxi cabs are an internationally recognized icon and serve to transport visitors to the city and residents alike, throughout the five boroughs. All Yellow Taxis are equipped with GPS monitoring systems that transmit data to a centralized database. \n",
    "\n",
    "This post will focus on Pyspark and how it can used to ingest ~10 gb worth of data, perform some analysis on them and even predict trip duration using Pyspark's ML libraries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries & Setting up Pyspark Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "## Setting up Pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import sql\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data\n",
    "NYC Taxi and Limousine Commission (TLC) has collected trip data which includes fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts. [Found here](http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml)\n",
    "\n",
    "For this analysis, data for Yellow taxi has been collected for a duration of 6 months (~10 GB)\n",
    "\n",
    "\n",
    "Six months of data were downloaded from the from the New York City Taxi Cab website and uploaded to the cluster. Each month was originally downloaded as a CSV file and transformed into an RDD. The six RDDs were then merged using an sc.union function to form one data set comprising all six months of data. A pyspark SQL context was then initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = sc.textFile('/data/yellow_tripdata_2016-01.csv')\n",
    "data2 = sc.textFile('/data/yellow_tripdata_2016-02.csv')\n",
    "data3 = sc.textFile('/data/yellow_tripdata_2016-03.csv')\n",
    "data4 = sc.textFile('/data/yellow_tripdata_2016-04.csv')\n",
    "data5 = sc.textFile('/data/yellow_tripdata_2016-05.csv')\n",
    "data6 = sc.textFile('/data/yellow_tripdata_2016-06.csv')\n",
    "myRDDlist = [data1,data2,data3,data4,data5,data6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "The RDD was converted to a spark sql dataframe to be used in analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dataframe(data):\n",
    "    trans = data.map(lambda x: x.encode(\"ascii\", \"ignore\"))\n",
    "    tagsheader = trans.first()\n",
    "    header = sc.parallelize([tagsheader])\n",
    "    trans_data = trans.subtract(header)\n",
    "    tuple_data = trans_data.map(lambda x: tuple(str(x).split(\",\")))\n",
    "    df = tuple_data.toDF([\"VendorID\",\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\",\"passenger_count\",\"trip_distance\",\\\n",
    "                 \"pickup_longitude\",\"pickup_latitude\",\"RatecodeID\",\"store_and_fwd_flag\",\"dropoff_longitude\",\\\n",
    "                 \"dropoff_latitude\",\"payment_type\",\"fare_amount\",\"extra\",\"mta_tax\",\"tip_amount\",\"tolls_amount\",\\\n",
    "                 \"improvement_surcharge\",\"total_amount\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "The datatypes of the columns need to be changed to float type. There are also trips greater than 20 miles which are discarded from the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_filter(df):\n",
    "\n",
    "    ## Converting to appropriate datatypes\n",
    "    df = df.withColumn('pickup_longitude', df['pickup_longitude'].cast('float'))\n",
    "    df = df.withColumn('pickup_latitude', df['pickup_latitude'].cast('float'))\n",
    "    df = df.withColumn('dropoff_longitude', df['dropoff_longitude'].cast('float'))\n",
    "    df = df.withColumn('dropoff_latitude', df['dropoff_latitude'].cast('float'))\n",
    "    df = df.withColumn('trip_distance', df['trip_distance'].cast('float'))\n",
    "    # removing Outliers ( Checked the distribution of the distance -- Removing distance greater than 20 miles)\n",
    "    df = df.filter(df.trip_distance<20.0)\n",
    "    ## Dropping null rows\n",
    "    df = df.na.drop()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enumerating all Data Frames into a single Data Frame\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "for _,rdd_no in enumerate(myRDDlist):\n",
    "    df = convert_to_dataframe(myRDDlist[rdd_no])\n",
    "    df = clean_and_filter(df)\n",
    "    final_df = final_df.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "#### Computing Average Speed\n",
    "\n",
    "Average traffic speed can be a good indicator of underlying traffic and can be a good predictor of trip duration. Two new variables were added; avg_speed and duration in minutes, for each observation by Trip_distance/(duration_in_mins).\n",
    "\n",
    "#### Additional Features\n",
    "Information about the weekday/weekend can also prove detrimental in determing the trip duration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computing Speed\n",
    "format = \"yyyy-MM-dd HH:mm:ss\"\n",
    "timeDiff = (F.unix_timestamp('tpep_dropoff_datetime', format)\n",
    "            - F.unix_timestamp('tpep_pickup_datetime', format))/60\n",
    "\n",
    "df = df.withColumn(\"tpep_pickup_datetime\", from_unixtime(unix_timestamp(df.tpep_pickup_datetime, \"yyyy-MM-dd HH:mm:ss\")))\n",
    "df = df.withColumn(\"pickup_hr\",hour(df.tpep_pickup_datetime))\n",
    "df = df.withColumn(\"Duration_in_mins\", timeDiff)\n",
    "df = df.withColumn(\"Speed_mph\",df.trip_distance/ ((df.Duration_in_mins)/60))\n",
    "df = df.withColumn(\"pickup_month\",month(df.tpep_pickup_datetime))\n",
    "\n",
    "import datetime as dt\n",
    "## Computing day of the week by using a User Defined Function\n",
    "def get_weekday(date):\n",
    "    import datetime\n",
    "    import calendar\n",
    "    date = date.split(' ')[0]\n",
    "    year,month,day = (int(x) for x in date.split('-'))    \n",
    "    weekday = datetime.date(year, month, day)\n",
    "    return calendar.day_name[weekday.weekday()]\n",
    "\n",
    "\n",
    "weekday_udf = udf(get_weekday)\n",
    "df = df.withColumn('pickup_day', weekday_udf(df.tpep_pickup_datetime))\n",
    "\n",
    "## Fititng K means Model \n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "### Converting lat long to float values\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "df = df.withColumn('pickup_longitude', df['pickup_longitude'].cast('float'))\n",
    "df = df.withColumn('pickup_latitude', df['pickup_latitude'].cast('float'))\n",
    "df = df.withColumn('dropoff_longitude', df['dropoff_longitude'].cast('float'))\n",
    "df = df.withColumn('dropoff_latitude', df['dropoff_latitude'].cast('float'))\n",
    "vecAssembler = VectorAssembler(inputCols=[\"dropoff_latitude\", \"dropoff_longitude\"], outputCol=\"features\")\n",
    "new_df = vecAssembler.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Clustering into zones\n",
    "Traffic and trip duration can also be attributed to the location in the city. Certain regions in the city can have high traffic and thus can be a good indicator of trip duration. Pyspark ML library has KMEANS clustering modules and that can be used to get clusters for our data.\n",
    "\n",
    "The cluster information is uesd to create additional features such as:\n",
    "1. pickup cluster\n",
    "2. dropoff cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(k=15, seed=1) \n",
    "model = kmeans.fit(new_df.select('features'))\n",
    "\n",
    "### Vecorizing and getting pickup clusters\n",
    "vecAssembler = VectorAssembler(inputCols=[\"pickup_latitude\", \"pickup_longitude\"], outputCol=\"features\")\n",
    "new_df = vecAssembler.transform(df)\n",
    "df = model.transform(new_df)\n",
    "\n",
    "## assigniung prediction to pickup cluster\n",
    "df = data1_df.withColumnRenamed('prediction', 'pickup_cluster')\n",
    "df = data1_df.drop('features')\n",
    "\n",
    "### Vecorizing and getting dropoff clusters\n",
    "vecAssembler = VectorAssembler(inputCols=[\"dropoff_latitude\", \"dropoff_longitude\"], outputCol=\"features\")\n",
    "new_df = vecAssembler.transform(df)\n",
    "df = model.transform(new_df)\n",
    "\n",
    "## assigniung prediction to dropoff cluster\n",
    "df = data1_df.withColumnRenamed('prediction', 'dropoff_cluster')\n",
    "df = data1_df.drop('features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the cluster\n",
    "\n",
    "The figure below the different clusters based on the lat/long of pickup co-ordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "% matplotlib inline\n",
    "### Visualizing the clusters\n",
    "pd_df = df.toPandas()\n",
    "pd_df = pd_df.sample(frac= 0.1)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.lmplot(x=\"pickup_latitude\", y=\"pickup_longitude\",data = pd_df[pd_df['pickup_latitude']!=0.0],fit_reg=False,hue='pickup_cluster',size=10,scatter_kws={\"s\":100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Trip Duration\n",
    "\n",
    "The features used for the prediction are converted to appropriated data types.\n",
    "Null values were dropped from the dataframe and then the data was split into training and test data sets\n",
    "\n",
    "A simple linear regression module of Pyspark's ML library was used to compute the Mean square errors and other measure parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('VendorID', df['VendorID'].cast('double'))\n",
    "df = df.withColumn('passenger_count', df['passenger_count'].cast('double'))\n",
    "df = df.withColumn('trip_distance', df['trip_distance'].cast('double'))\n",
    "df = df.withColumn('RatecodeID', df['RatecodeID'].cast('double'))\n",
    "df = df.withColumn('store_and_fwd_flag', df['store_and_fwd_flag'].cast('double'))\n",
    "df = df.withColumn('payment_type', df['payment_type'].cast('double'))\n",
    "df = df.withColumn('fare_amount', df['fare_amount'].cast('double'))\n",
    "df = df.withColumn('extra', df['extra'].cast('double'))\n",
    "df = df.withColumn('mta_tax', df['mta_tax'].cast('double'))\n",
    "df = df.withColumn('tip_amount', df['tip_amount'].cast('double'))\n",
    "df = df.withColumn('tolls_amount', df['tolls_amount'].cast('double'))\n",
    "df = df.withColumn('improvement_surcharge', df['improvement_surcharge'].cast('double'))\n",
    "df = df.withColumn('total_amount', df['total_amount'].cast('double'))\n",
    "## Importing ML libraries\n",
    "from pyspark.ml.regression import LinearRegression  \n",
    "from pyspark.ml.feature import VectorAssembler  \n",
    "from pyspark.ml.feature import StandardScaler  \n",
    "from pyspark.ml import Pipeline  \n",
    "from pyspark.sql.functions import *  \n",
    "\n",
    "features = ['passenger_count', 'trip_distance', \\\n",
    "            'RatecodeID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount',\\\n",
    "            'improvement_surcharge', 'pickup_cluster', 'dropoff_cluster', 'pickup_hr', 'Speed_mph']  \n",
    "lr_data = df.select(col(\"Duration_in_mins\").alias(\"label\"), *features) \n",
    "\n",
    "## check for Null Values and dropping null values if any\n",
    "for f in features:\n",
    "    print (f)\n",
    "    print (lr_data.where(col(f).isNull()).count())\n",
    "    \n",
    "lr_data = lr_data.dropna()\n",
    "\n",
    "(training, test) = lr_data.randomSplit([.7, .3])\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=features, outputCol=\"unscaled_features\")  \n",
    "standardScaler = StandardScaler(inputCol=\"unscaled_features\", outputCol=\"features\")  \n",
    "lr = LinearRegression(maxIter=10, regParam=.01)\n",
    "\n",
    "stages = [vectorAssembler, standardScaler, lr]  \n",
    "pipeline = Pipeline(stages=stages) \n",
    "\n",
    "model = pipeline.fit(training)  \n",
    "prediction = model.transform(test)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator  \n",
    "eval = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Root Mean Square Error\n",
    "rmse = eval.evaluate(prediction)  \n",
    "print(\"RMSE: %.3f\" % rmse)\n",
    "\n",
    "# Mean Square Error\n",
    "mse = eval.evaluate(prediction, {eval.metricName: \"mse\"})  \n",
    "print(\"MSE: %.3f\" % mse)\n",
    "\n",
    "# Mean Absolute Error\n",
    "mae = eval.evaluate(prediction, {eval.metricName: \"mae\"})  \n",
    "print(\"MAE: %.3f\" % mae)\n",
    "\n",
    "# r2 - coefficient of determination\n",
    "r2 = eval.evaluate(prediction, {eval.metricName: \"r2\"})  \n",
    "print(\"r2: %.3f\" %r2)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
